# LLM-Based-Code_Review_Agent
## 项目简介
我在 2025 年 1-4 月主导开发了 “基于 GPT API 的自动代码审核 Agent”，核心是解决团队手动代码审核 “效率低、标准不统一、易漏检” 的痛点 —— 当时人工审核 100 行 Python 代码要 15 分钟，且不同审核人员对 PEP8 规范理解有差异，还常忽略隐性逻辑漏洞。
项目整体是 “上传代码→AI 分析→输出报告” 的全流程：用户可直接粘贴代码或上传.py 文件，Agent 会先预处理（清洗格式、按函数分片），再调用 GPT API（基础代码用 GPT-3.5 控成本，复杂代码用 GPT-4o 保精度），基于 PEP8 规范做 “样式检查（命名、缩进等）+ 逻辑分析（异常处理、资源泄漏等）”，最后输出带 “问题分级 + 修改对比” 的 Markdown/PDF 报告。
为适配不同团队需求，还支持 3 种 PEP8 策略切换（严格 / 宽松 / 自定义），比如大型团队可选严格模式，创业团队可放宽行宽限制。
最终效果很明显：团队代码审核效率提升 73%，100 行代码审核从 15 分钟缩到 2 分钟；代码规范不符合率从 25% 降到 8%，也减少了因规范问题的沟通成本。
## 一、项目核心定位与背景深挖
### 为什么要做“自动代码审核Agent”？解决了什么具体痛点？
- 手动审核的核心痛点（需结合实际场景，体现项目必要性）：  
  - 效率低：团队代码审核平均每100行Python代码需15-20分钟（人工逐行检查PEP8样式、逻辑漏洞），若迭代周期短（如2周1个版本），审核压力大；  
  - 标准不统一：不同审核人员对PEP8规范的理解有差异（如函数命名是否严格用蛇形命名法、注释占比是否达标），导致同一代码多次修改；  
  - 漏检风险：人工易忽略“隐性问题”（如未关闭的文件句柄、列表遍历中的索引越界隐患、未使用的导入包），上线后可能引发Bug。  
- Agent的定位：不是替代人工审核，而是“前置过滤+辅助提效”——先由Agent完成80%的基础审核（样式、基础逻辑），人工聚焦20%的复杂场景（业务逻辑对齐、架构合理性），形成“人机协同”的审核模式。
- 项目阶段适配：项目周期仅3个月（01/2025-04/2025），需快速验证“自动审核”的可行性，选择GPT API能以最低成本实现核心功能，若后续落地可再评估开源模型的本地化方案（平衡成本与数据隐私）。

## 二、技术细节（从“功能设计→实现→评估”全流程拆解）
### 1. Agent的核心架构：如何实现“上传代码→分析→输出报告”的全流程？
需拆解为“输入层、分析层、输出层”三个模块，明确各模块的技术实现与逻辑：  
（1）输入层：代码上传与预处理（解决“格式统一、信息补充”问题）
- 上传方式：支持两种输入形式（贴合用户习惯）：  
  - 文本框直接粘贴代码片段（适合小文件，如单个Python函数）；  
  - 文件上传（支持.py文件，通过python-magic库解析文件类型，拒绝非Python文件）；  
- 预处理逻辑（避免模型分析时受干扰）：  
  1. 格式清洗：去除代码中的注释符号（#单行注释、"""多行注释）——避免注释内容干扰模型对代码逻辑的判断（如注释中的“TODO”被误判为未完成代码）；  
  2. 信息补充：自动添加“代码元信息”（如“该代码为Python工具类，功能是数据格式转换”）——通过Prompt引导模型聚焦核心场景，减少无意义分析；  
  3. 分片处理：对超过200行的长代码，按“函数/类”拆分（如用ast模块解析抽象语法树，提取def/class块）——避免单次输入token超GPT API限制（如GPT-4o上下文窗口虽大，但分片后分析更精准）。
（2）分析层：GPT API调用与审核逻辑设计（核心技术点）
- Prompt设计：如何让模型精准执行“样式检查+综合分析”？需采用“指令清晰化+Few-Shot示例”的Prompt策略，示例Prompt结构：  
你是Python代码审核专家，需基于PEP8规范和Python最佳实践，完成以下审核任务：
1. 样式检查：重点检查①函数/变量命名（是否蛇形命名）、②缩进（4个空格）、③空行（函数间空2行）、④导入顺序（标准库→第三方库→自定义库）；
2. 综合分析：检查①逻辑漏洞（如未处理的异常、索引越界）、②性能问题（如循环中重复创建列表）、③可读性问题（如无意义变量名）；
3. 输出要求：按“问题类型-问题位置-修改建议”格式输出，无问题则标注“无待优化项”。

示例1：
输入代码：
def calculateTotal(a,b):
    total=a+b
    return total
审核结果：
1. 样式问题-函数命名：calculateTotal应为calculate_total（PEP8蛇形命名）；
2. 样式问题-缩进：total=a+b未缩进4个空格；
3. 无逻辑/性能问题。

示例2：
输入代码：
import pandas as pd
import os
def read_data(file_path):
    f = open(file_path, 'r')
    data = f.read()
    return data
审核结果：
1. 样式问题-导入顺序：os（标准库）应在pandas（第三方库）之前；
2. 逻辑问题-文件句柄：未关闭f，可能导致资源泄漏，建议用with open(...) as f:；
3. 无性能问题。

请审核以下代码：
[预处理后的用户代码]
  - 设计逻辑：通过“明确审核维度”（样式4点+综合3点）避免模型遗漏；通过“2个示例”让模型学习输出格式，减少杂乱回复；  
  - 优化点：加入“拒绝无关分析”指令（如“不讨论代码的业务逻辑合理性，仅关注语法、样式、性能”），避免模型输出超出范围的内容。
- API调用细节：如何平衡“精度”与“成本”？  
  1. 模型选择：基础审核用GPT-3.5-turbo（成本低，0.5美元/百万token），复杂代码（如包含装饰器、多线程）用GPT-4o（精度高，成本约3美元/百万token）——通过代码行数自动切换（如>100行用GPT-4o）；  
  2. 批量调用：若用户上传多文件（如一个Python项目的3个.py文件），用GPT API的“批量请求”功能（batch_create），减少API调用次数，降低延迟；  
  3. 缓存机制：对相同代码（如用户重复上传同一文件），缓存审核结果（Redis存储，key为代码MD5值，过期时间1小时）——避免重复调用API，节省成本。
（3）输出层：审核报告生成（如何让结果“易懂、可落地”？）
- 报告格式设计（分“概览+详情”，贴合团队审核习惯）：  
  1. 概览部分：统计“问题总数”“样式问题数”“逻辑问题数”“无问题文件数”——让审核人员快速掌握整体情况（如“3个文件共5个问题，其中3个样式问题，2个逻辑问题”）；  
  2. 详情部分：按“文件→问题类型”分级展示，每个问题附带“修改前后对比”（示例）：  
文件名
问题类型
问题位置
修改建议
修改后示例
data_utils.py
样式问题
第5行函数命名
getdata应为get_data（PEP8蛇形命名）
def get_data(file_path):

逻辑问题
第12行文件操作
未关闭文件句柄，用with open
with open(file_path, 'r') as f:
  - 附加功能：支持“一键复制修改建议”“导出报告为Markdown/PDF”——方便审核人员同步给开发同学，减少沟通成本。
2. 支持“多种符合PEP8规范的策略”具体是指什么？如何实现策略切换？
- 策略定义：PEP8规范中部分规则存在“可选场景”，项目支持用户自定义“严格/宽松/自定义”三种策略，满足不同团队的审核标准：  
策略类型
核心规则差异
适用场景
严格策略
1. 必须用蛇形命名（无例外）；2. 导入必须按“标准库→第三方→自定义”排序；3. 每行代码不超过79字符
对代码规范要求高的团队（如开源项目、大型企业）
宽松策略
1. 类名允许帕斯卡命名（如DataUtils）；2. 导入顺序可灵活调整；3. 每行代码允许不超过120字符
小型团队、快速迭代项目（如创业公司）
自定义策略
用户可勾选“需检查的规则”（如仅检查命名+缩进，不检查空行）
个性化需求团队（如仅关注核心样式问题）
- 实现逻辑：策略配置→Prompt注入→结果过滤  
  1. 策略配置：前端提供“策略选择界面”（下拉框+勾选框），用户选择后将策略参数（如strict=True、check_indent=True）传入后端；  
  2. Prompt注入：后端根据策略参数动态修改Prompt中的“审核规则”，例如：  
    - 严格策略：Prompt中添加“所有命名必须严格遵循蛇形命名，不允许例外”；  
    - 宽松策略：Prompt中添加“类名允许帕斯卡命名，函数名必须蛇形命名”；  
  3. 结果过滤：若用户选择“不检查空行”，则在模型输出结果后，自动过滤“空行相关问题”，确保最终报告符合用户策略选择。
3. “对模型输出进行效果评估”具体怎么做？评估指标是什么？如何基于评估优化Agent？
（1）评估数据集构建（确保覆盖多样化场景）
- 数据来源：收集团队过往的“人工审核记录”（400个Python代码文件，包含500+个已知问题），按“样式问题（60%）+逻辑问题（40%）”分类标注；  
- 数据划分：80个文件作为“评估集”（用于测试Agent效果），20个文件作为“优化集”（用于分析Agent漏检/误检原因）。
（2）评估指标设计（量化“审核效果”与“提效能力”）
- 核心指标1：审核准确率（Precision/Recall/F1）（衡量Agent是否精准）：  
  - 精确率（Precision）：Agent标注的“问题”中，真正为问题的比例（避免误判，如将正确代码标为问题）；  
  - 召回率（Recall）：人工标注的“问题”中，Agent成功检测到的比例（避免漏检，如未发现代码中的逻辑漏洞）；  
  - 目标值：F1分数≥0.85（样式问题F1≥0.9，逻辑问题F1≥0.8）——因为逻辑问题更复杂，允许稍低的召回率。  
- 核心指标2：审核效率提升比（衡量“提效”是否达标）：  
  - 计算方式：（人工审核平均耗时 - Agent审核平均耗时）/ 人工审核平均耗时 × 100%；  
  - 数据结果：人工审核1个100行代码文件平均需15分钟，Agent仅需2分钟，效率提升73.3%。  
- 辅助指标：用户满意度（通过问卷收集）：  
  - 调研维度：“问题描述清晰度”“修改建议可落地性”“报告易读性”；  
  - 目标值：满意度评分≥4.2/5分。
（3）基于评估的优化动作（体现迭代思维）
- 问题1：逻辑问题召回率低（如未检测到“未处理的KeyError”）  
  - 原因分析：GPT-3.5对“隐性异常处理”的敏感度不足，示例Prompt中未覆盖该场景；  
  - 优化方案：在Prompt中新增“异常处理检查”示例（如“输入代码：d = {'a':1}; print(d['b']) → 审核结果：逻辑问题-未处理KeyError，建议用get()方法或try-except”），同时升级复杂代码的模型为GPT-4o；  
  - 优化效果：逻辑问题召回率从0.75提升至0.83。  
- 问题2：宽松策略下误判率高（如将帕斯卡命名的类标为样式问题）  
  - 原因分析：Prompt中策略参数注入不彻底，模型仍按默认严格策略审核；  
  - 优化方案：在Prompt开头明确标注“当前策略：宽松策略，允许类名帕斯卡命名”，并在示例中加入帕斯卡命名的正确案例；  
  - 优化效果：宽松策略下误判率从0.15降至0.05。
## 三、项目难点与解决方案
1. 难点1：长代码分析时，模型易遗漏上下文关联（如函数间参数传递错误）
- 问题描述：当代码超过300行（如包含多个函数调用），模型分片分析后，可能忽略“函数A的返回值类型与函数B的参数类型不匹配”这类跨分片问题（如函数A返回list，函数B需接收dict）；  
- 解决方案：上下文关联补充+跨分片校验：  
  1. 分片时保留“函数签名信息”（如def func_a() -> list），每个分片的Prompt中附带“所有函数的签名列表”——让模型知道其他函数的输入输出类型；  
  2. 所有分片分析完成后，新增“跨分片校验步骤”：调用GPT API，输入“所有分片的问题列表+完整函数签名”，让模型判断是否存在“跨函数逻辑问题”；  
- 效果：跨分片问题的召回率从0.3提升至0.75，例如成功检测到“func_a返回list，func_b接收dict”的参数类型不匹配问题。
2. 难点2：API调用成本超出预期（初期测试100个文件成本达50美元）
- 问题描述：初期所有代码都用GPT-4o审核，且无缓存机制，导致成本过高（项目预算仅200美元，无法支撑大量测试）；  
- 解决方案：分级模型+缓存+成本监控：  
  1. 分级模型：按代码复杂度（行数+语法复杂度）自动选择模型——≤100行且无复杂语法（如装饰器）用GPT-3.5，>100行或有复杂语法用GPT-4o；  
  2. 强化缓存：对相同代码（MD5一致）、相同策略下的审核结果，缓存过期时间从1小时延长至24小时；同时缓存“模型输出的原始结果”，避免重复生成报告；  
  3. 成本监控：开发“成本统计脚本”，实时记录API调用次数、token消耗、金额，当单日成本超20美元时触发告警，及时调整策略；  
- 效果：100个文件的审核成本从50美元降至12美元，预算可支撑2000+个文件的审核。
3. 难点3：不同团队对“优化建议”的接受度差异大（如部分团队认为建议太苛刻）
- 问题描述：Agent按PEP8严格输出建议，但部分团队（如快速迭代的创业团队）认为“部分建议无必要”（如要求每行不超过79字符，团队习惯120字符），导致用户满意度低；  
- 解决方案：建议分级+自定义权重：  
  1. 建议分级：将问题分为“必须修改（红色，如逻辑漏洞）”“推荐修改（黄色，如非关键样式问题）”“可选修改（绿色，如空行数量）”——让团队优先处理高优先级问题；  
  2. 自定义权重：允许团队在“自定义策略”中设置“问题优先级权重”（如将“空行问题”权重设为0，即不显示该类建议）；  
- 效果：用户满意度从3.8分提升至4.5分，80%的团队反馈“建议更贴合实际需求”。
## 四、项目价值与个人贡献
### 1. 项目价值（技术+业务双维度）
- 技术价值：  
  1. 验证了“LLM Agent+代码审核”的可行性，提供了“Prompt设计→效果评估→成本优化”的完整技术方案，可复用于其他静态代码分析场景（如Java/Go代码审核）；  
  2. 提出的“分级模型+策略切换”方案，为同类LLM应用（如自动文档生成、智能测试）提供了“精度-成本-用户体验”平衡的参考。  
- 业务价值：  
  1. 效率提升：团队代码审核效率提升73.3%，若按10人团队、每人每周审核10个文件计算，每周可节省10×10×(15-2)=1300分钟（约21.7小时），释放人力投入核心开发；  
  2. 规范统一：通过标准化审核策略，团队代码规范不符合率从25%降至8%，减少因规范问题导致的代码冲突和维护成本。
### 2. 个人核心贡献
1. 核心架构设计：独立完成Agent“输入-分析-输出”三层架构设计，主导Prompt的“指令+Few-Shot”策略优化，将审核准确率（F1）从0.72提升至0.88；  
2. 关键功能落地：  
  - 实现“多种PEP8策略切换”功能，开发前端策略配置界面与后端Prompt动态注入逻辑，支持3类策略、12个可配置规则；  
  - 搭建“效果评估体系”，构建100个文件的评估数据集，设计Precision/Recall/效率提升比等指标，完成3轮优化迭代；  
3. 难点攻坚：独立解决“长代码跨分片分析”和“API成本超支”两个核心问题，跨分片问题召回率提升150%，成本降低76%；  
4. 工程化交付：负责审核报告的Markdown/PDF导出功能开发，编写项目技术文档（含API调用指南、策略配置说明），支持3个团队的内部试用。
## 五、Q&A

“你在设计Prompt时，为什么选择这2个示例？有没有尝试过更多示例？”
示例选择逻辑→尝试过的方案→最终选择的原因
“选择这2个示例是因为：示例1覆盖‘基础样式问题’（命名+缩进），示例2覆盖‘样式+逻辑问题’（导入顺序+文件句柄），能覆盖80%的常见场景。初期尝试过4个示例，但发现模型输出会过度模仿示例细节（如只关注示例中的问题类型，忽略其他问题），后来缩减到2个，既保证模型理解格式，又不限制分析范围，最终F1分数反而从0.82提升到0.88。”

“这个项目和市面上的代码审核工具（如pylint）相比，有什么优势和不足？”

“优势在于：1. 传统pylint只能检测样式问题，我们的Agent能同时检测逻辑问题（如未处理的异常）和性能问题（如循环低效），覆盖范围更广；2. pylint的错误提示较生硬（如‘E1101: Instance of 'int' has no 'append' member’），我们的Agent能提供‘修改前后对比’，更易理解。不足在于：1. 成本更高（pylint免费，我们的Agent需API费用）；2. 离线环境无法使用（依赖GPT API）。后续改进方向是‘混合模式’：基础样式问题用pylint（免费+快速），复杂逻辑问题用LLM Agent，平衡成本与精度。”
